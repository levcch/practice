{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "id": "Wu_yDhjegJ-F",
        "outputId": "01155d57-e65b-41ae-cda0-7d2b293fcff8"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'keras_tuner'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-5f8146053529>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mkeras_tuner\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mkt\u001b[0m \u001b[0;31m# Импортируем KerasTuner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras_tuner'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import keras_tuner as kt # Импортируем KerasTuner\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 0. Проверка и установка KerasTuner, если необходимо\n",
        "# !pip install keras-tuner -q # Раскомментируйте, если KerasTuner не установлен\n",
        "\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"KerasTuner version: {kt.__version__}\")\n",
        "\n",
        "# 1. Загрузка и подготовка данных (Fashion MNIST)\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "# Нормализация изображений\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "\n",
        "# Добавление канала (т.к. CNN ожидает 4D тензор: [batch, height, width, channels])\n",
        "x_train = np.expand_dims(x_train, -1)\n",
        "x_test = np.expand_dims(x_test, -1)\n",
        "\n",
        "print(f\"x_train shape: {x_train.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"x_test shape: {x_test.shape}\")\n",
        "print(f\"y_test shape: {y_test.shape}\")\n",
        "\n",
        "num_classes = 10\n",
        "input_shape = x_train.shape[1:] # (28, 28, 1)\n",
        "\n",
        "# 2. Создание функции для построения модели (hypermodel)\n",
        "def build_model(hp):\n",
        "    model = keras.Sequential()\n",
        "    model.add(layers.Input(shape=input_shape))\n",
        "\n",
        "    # Тюнинг количества фильтров в первом сверточном слое\n",
        "    hp_filters1 = hp.Int('filters_1', min_value=32, max_value=128, step=32)\n",
        "    model.add(layers.Conv2D(filters=hp_filters1, kernel_size=(3, 3), activation=\"relu\"))\n",
        "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Тюнинг количества фильтров во втором сверточном слое\n",
        "    # hp_filters2 = hp.Int('filters_2', min_value=32, max_value=64, step=16) # Можно добавить больше слоев\n",
        "    # model.add(layers.Conv2D(filters=hp_filters2, kernel_size=(3, 3), activation=\"relu\"))\n",
        "    # model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "\n",
        "    # Тюнинг количества нейронов в полносвязном слое\n",
        "    hp_units = hp.Int('units', min_value=32, max_value=512, step=32)\n",
        "    model.add(layers.Dense(units=hp_units, activation=\"relu\"))\n",
        "\n",
        "    # Тюнинг коэффициента Dropout\n",
        "    hp_dropout_rate = hp.Float('dropout_rate', min_value=0.0, max_value=0.5, step=0.1)\n",
        "    model.add(layers.Dropout(rate=hp_dropout_rate))\n",
        "\n",
        "    model.add(layers.Dense(num_classes, activation=\"softmax\"))\n",
        "\n",
        "    # Тюнинг скорости обучения\n",
        "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
        "        loss=\"sparse_categorical_crossentropy\", # Используем sparse, т.к. y_train - это индексы классов\n",
        "        metrics=[\"accuracy\"],\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# 3. Настройка и запуск KerasTuner\n",
        "# Мы будем использовать RandomSearch. Другие опции: Hyperband, BayesianOptimization\n",
        "tuner = kt.RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_accuracy', # Метрика для оптимизации\n",
        "    max_trials=10,  # Количество комбинаций гиперпараметров для проверки (увеличьте для реальных задач)\n",
        "    executions_per_trial=1, # Сколько раз обучать каждую модель (для стабильности, увеличьте для реальных задач)\n",
        "    directory='keras_tuner_dir', # Директория для сохранения результатов\n",
        "    project_name='fashion_mnist_tuning'\n",
        ")\n",
        "\n",
        "# Выводим информацию о пространстве поиска\n",
        "tuner.search_space_summary()\n",
        "\n",
        "# Определяем колбэк для ранней остановки, чтобы не тратить время на плохие конфигурации\n",
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "\n",
        "# Запускаем поиск лучших гиперпараметров\n",
        "# Используем validation_split для создания валидационной выборки из обучающей\n",
        "print(\"\\nStarting hyperparameter search...\")\n",
        "tuner.search(\n",
        "    x_train, y_train,\n",
        "    epochs=10, # Количество эпох для каждой *пробной* модели (увеличьте для реальных задач)\n",
        "    validation_split=0.2,\n",
        "    callbacks=[stop_early],\n",
        "    verbose=1 # Можно поставить 0 или 2 для меньшего вывода\n",
        ")\n",
        "\n",
        "# Выводим лучшие найденные гиперпараметры\n",
        "print(\"\\n--- Best Hyperparameters ---\")\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "print(f\"\"\"\n",
        "The hyperparameter search is complete.\n",
        "Optimal number of filters in the first conv layer: {best_hps.get('filters_1')}\n",
        "Optimal number of units in the first dense layer: {best_hps.get('units')}\n",
        "Optimal dropout rate: {best_hps.get('dropout_rate'):.2f}\n",
        "Optimal learning rate for the optimizer: {best_hps.get('learning_rate')}\n",
        "\"\"\")\n",
        "\n",
        "# Можно также получить лучшую модель напрямую\n",
        "best_model_from_tuner = tuner.get_best_models(num_models=1)[0]\n",
        "print(\"\\nSummary of the best model from tuner:\")\n",
        "best_model_from_tuner.summary()\n",
        "\n",
        "\n",
        "# 4. Обучение лучшей модели\n",
        "# Теперь построим модель с лучшими гиперпараметрами и обучим ее на большем количестве эпох\n",
        "print(\"\\n--- Training the best model ---\")\n",
        "model = tuner.hypermodel.build(best_hps) # Строим модель с лучшими hp\n",
        "\n",
        "# Можно использовать весь x_train для обучения, если уверены, или оставить validation_split\n",
        "# Для финального обучения можно объединить train и validation данные, если вы уверены\n",
        "# в гиперпараметрах, и обучать на (x_train + x_val), а тестировать на x_test.\n",
        "# Здесь для простоты оставим validation_split.\n",
        "\n",
        "history = model.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=30, # Обучаем дольше\n",
        "    validation_split=0.2,\n",
        "    callbacks=[stop_early] # Также используем раннюю остановку\n",
        ")\n",
        "\n",
        "# 5. Оценка модели на тестовых данных\n",
        "print(\"\\n--- Evaluating the best model on test data ---\")\n",
        "loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# 6. Визуализация истории обучения (опционально)\n",
        "def plot_history(history_obj):\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history_obj.history['accuracy'], label='Train Accuracy')\n",
        "    plt.plot(history_obj.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.title('Model Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history_obj.history['loss'], label='Train Loss')\n",
        "    plt.plot(history_obj.history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Model Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_history(history)\n",
        "\n",
        "# Сохранение лучшей модели (опционально)\n",
        "# model.save(\"best_fashion_mnist_model.h5\")\n",
        "# print(\"\\nBest model saved as best_fashion_mnist_model.h5\")\n",
        "\n",
        "# Посмотреть результаты поиска KerasTuner можно командой (в терминале):\n",
        "# tensorboard --logdir keras_tuner_dir/fashion_mnist_tuning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xAwlBCYd-svW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0afut_--wcq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FcPr14nm9JCi",
        "outputId": "898130b7-0909-448a-df61-7001be537b62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/129.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m92.2/129.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install keras-tuner -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-3GqC2-9PLu"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import keras_tuner as kt # Импортируем KerasTuner\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrKb8DPi9QUe",
        "outputId": "ec386bb8-28df-4d6e-d6f0-3a2604a4b4a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow version: 2.18.0\n",
            "KerasTuner version: 1.4.7\n"
          ]
        }
      ],
      "source": [
        "# 0. Проверка и установка KerasTuner, если необходимо\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"KerasTuner version: {kt.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSnLgbB69Z3j",
        "outputId": "79d2268c-f03e-4e56-9f9f-9c1c8d4a067d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "x_train shape: (60000, 28, 28, 1)\n",
            "y_train shape: (60000,)\n",
            "x_test shape: (10000, 28, 28, 1)\n",
            "y_test shape: (10000,)\n"
          ]
        }
      ],
      "source": [
        "# 1. Загрузка и подготовка данных (Fashion MNIST)\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "# Нормализация изображений\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "\n",
        "# Добавление канала (т.к. CNN ожидает 4D тензор: [batch, height, width, channels])\n",
        "x_train = np.expand_dims(x_train, -1)\n",
        "x_test = np.expand_dims(x_test, -1)\n",
        "\n",
        "print(f\"x_train shape: {x_train.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"x_test shape: {x_test.shape}\")\n",
        "print(f\"y_test shape: {y_test.shape}\")\n",
        "\n",
        "num_classes = 10\n",
        "input_shape = x_train.shape[1:] # (28, 28, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OzPA6AU89aBK"
      },
      "outputs": [],
      "source": [
        "# 2. Создание функции для построения модели (hypermodel)\n",
        "def build_model(hp):\n",
        "    model = keras.Sequential()\n",
        "    model.add(layers.Input(shape=input_shape))\n",
        "\n",
        "    # Тюнинг количества фильтров в первом сверточном слое\n",
        "    hp_filters1 = hp.Int('filters_1', min_value=32, max_value=128, step=32)\n",
        "    model.add(layers.Conv2D(filters=hp_filters1, kernel_size=(3, 3), activation=\"relu\"))\n",
        "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Тюнинг количества фильтров во втором сверточном слое\n",
        "    # hp_filters2 = hp.Int('filters_2', min_value=32, max_value=64, step=16) # Можно добавить больше слоев\n",
        "    # model.add(layers.Conv2D(filters=hp_filters2, kernel_size=(3, 3), activation=\"relu\"))\n",
        "    # model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "\n",
        "    # Тюнинг количества нейронов в полносвязном слое\n",
        "    hp_units = hp.Int('units', min_value=32, max_value=512, step=32)\n",
        "    model.add(layers.Dense(units=hp_units, activation=\"relu\"))\n",
        "\n",
        "    # Тюнинг коэффициента Dropout\n",
        "    hp_dropout_rate = hp.Float('dropout_rate', min_value=0.0, max_value=0.5, step=0.1)\n",
        "    model.add(layers.Dropout(rate=hp_dropout_rate))\n",
        "\n",
        "    model.add(layers.Dense(num_classes, activation=\"softmax\"))\n",
        "\n",
        "    # Тюнинг скорости обучения\n",
        "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
        "        loss=\"sparse_categorical_crossentropy\", # Используем sparse, т.к. y_train - это индексы классов\n",
        "        metrics=[\"accuracy\"],\n",
        "    )\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpJ6LV3W9aD-"
      },
      "outputs": [],
      "source": [
        "# 3. Настройка и запуск KerasTuner\n",
        "# Мы будем использовать RandomSearch. Другие опции: Hyperband, BayesianOptimization\n",
        "tuner = kt.RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_accuracy', # Метрика для оптимизации\n",
        "    max_trials=10,  # Количество комбинаций гиперпараметров для проверки (увеличьте для реальных задач)\n",
        "    executions_per_trial=1, # Сколько раз обучать каждую модель (для стабильности, увеличьте для реальных задач)\n",
        "    directory='keras_tuner_dir', # Директория для сохранения результатов\n",
        "    project_name='fashion_mnist_tuning'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWeDGeCD9QND",
        "outputId": "5ee900a1-0ab7-4c01-fbdd-5bb35e81d95a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial 10 Complete [00h 32m 47s]\n",
            "val_accuracy: 0.9169999957084656\n",
            "\n",
            "Best val_accuracy So Far: 0.922166645526886\n",
            "Total elapsed time: 03h 04m 00s\n"
          ]
        }
      ],
      "source": [
        "# Выводим информацию о пространстве поиска\n",
        "tuner.search_space_summary()\n",
        "\n",
        "# Определяем колбэк для ранней остановки, чтобы не тратить время на плохие конфигурации\n",
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "\n",
        "# Запускаем поиск лучших гиперпараметров\n",
        "# Используем validation_split для создания валидационной выборки из обучающей\n",
        "print(\"\\nStarting hyperparameter search...\")\n",
        "tuner.search(\n",
        "    x_train, y_train,\n",
        "    epochs=10, # Количество эпох для каждой *пробной* модели (увеличьте для реальных задач)\n",
        "    validation_split=0.2,\n",
        "    callbacks=[stop_early],\n",
        "    verbose=1 # Можно поставить 0 или 2 для меньшего вывода\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXbHE3f89QFh"
      },
      "outputs": [],
      "source": [
        "# Выводим лучшие найденные гиперпараметры\n",
        "print(\"\\n--- Best Hyperparameters ---\")\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "print(f\"\"\"\n",
        "The hyperparameter search is complete.\n",
        "Optimal number of filters in the first conv layer: {best_hps.get('filters_1')}\n",
        "Optimal number of units in the first dense layer: {best_hps.get('units')}\n",
        "Optimal dropout rate: {best_hps.get('dropout_rate'):.2f}\n",
        "Optimal learning rate for the optimizer: {best_hps.get('learning_rate')}\n",
        "\"\"\")\n",
        "\n",
        "# Можно также получить лучшую модель напрямую\n",
        "best_model_from_tuner = tuner.get_best_models(num_models=1)[0]\n",
        "print(\"\\nSummary of the best model from tuner:\")\n",
        "best_model_from_tuner.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqFSpVcN9P4T"
      },
      "outputs": [],
      "source": [
        "# 4. Обучение лучшей модели\n",
        "# Теперь построим модель с лучшими гиперпараметрами и обучим ее на большем количестве эпох\n",
        "print(\"\\n--- Training the best model ---\")\n",
        "model = tuner.hypermodel.build(best_hps) # Строим модель с лучшими hp\n",
        "\n",
        "# Можно использовать весь x_train для обучения, если уверены, или оставить validation_split\n",
        "# Для финального обучения можно объединить train и validation данные, если вы уверены\n",
        "# в гиперпараметрах, и обучать на (x_train + x_val), а тестировать на x_test.\n",
        "# Здесь для простоты оставим validation_split.\n",
        "\n",
        "history = model.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=30, # Обучаем дольше\n",
        "    validation_split=0.2,\n",
        "    callbacks=[stop_early] # Также используем раннюю остановку\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p18JjFLV-x9m"
      },
      "outputs": [],
      "source": [
        "# 5. Оценка модели на тестовых данных\n",
        "print(\"\\n--- Evaluating the best model on test data ---\")\n",
        "loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# 6. Визуализация истории обучения (опционально)\n",
        "def plot_history(history_obj):\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history_obj.history['accuracy'], label='Train Accuracy')\n",
        "    plt.plot(history_obj.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.title('Model Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history_obj.history['loss'], label='Train Loss')\n",
        "    plt.plot(history_obj.history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Model Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_history(history)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}